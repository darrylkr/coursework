{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset 'yoochoose-clicks' is a set of click events collected from a website of an online retailer. Each record in the dataset has 4 fields:\n",
    "\n",
    "Session ID - the id of the session. In one session there are one or many clicks.  \n",
    "Timestamp - the time when the click occurred.  \n",
    "Item ID – the unique identifier of item.  \n",
    "Category – the category of the item.  \n",
    "\n",
    "The value \"S\" indicates a special offer, \"0\" indicates a missing value, a number between 1 to 12 indicates a\n",
    "real category identifier, and any other number indicates a brand. If an item has been clicked in the context of\n",
    "a promotion or special offer then the value is \"S\". If the category is a brand (i.e., BOSCH) then the value is an\n",
    "8-10 digits number.\n",
    "\n",
    "The objective is to **compute the average time** that users stay on items in each category.\n",
    "\n",
    "For analysis purposes in this task, the following definitions will be used:  \n",
    "(i) There are 15 item categories in the dataset: S, 0, 1 to 12, and B (for any 8-10 digits number)  \n",
    "(ii) In each session, the time that a user stays on some item is the timestamp difference between a user\n",
    "clicking on this item and the next item (if there is a next item)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T21:57:40.744494Z",
     "start_time": "2022-06-12T21:57:23.723099Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config(\"spark-master\", \"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T21:57:40.759010Z",
     "start_time": "2022-06-12T21:57:40.749244Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T21:57:40.977461Z",
     "start_time": "2022-06-12T21:57:40.765991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2014-04-07T10:51:09.277Z,214536502,0\n",
      "1,2014-04-07T10:54:09.868Z,214536500,0\n",
      "1,2014-04-07T10:54:46.998Z,214536506,0\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 data/yoochoose-clicks.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T21:57:41.010341Z",
     "start_time": "2022-06-12T21:57:40.984414Z"
    }
   },
   "outputs": [],
   "source": [
    "myManualSchema = StructType([\n",
    "    StructField(\"Session_ID\", IntegerType(), nullable=False),\n",
    "    StructField(\"Timestamp\", TimestampType(), nullable=False),\n",
    "    StructField(\"Item_ID\", IntegerType(), nullable=False),\n",
    "    StructField(\"Category\", StringType(), nullable=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get data types for schema by analyzing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T21:57:44.658891Z",
     "start_time": "2022-06-12T21:57:41.018320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Session_ID: integer (nullable = true)\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      " |-- Item_ID: integer (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_RD = spark.read.format(\"csv\").load(\"data/yoochoose-clicks.dat\", schema=myManualSchema)\n",
    "df_RD.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T21:57:48.729746Z",
     "start_time": "2022-06-12T21:57:44.663720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+---------+--------+\n",
      "|Session_ID|Timestamp              |Item_ID  |Category|\n",
      "+----------+-----------------------+---------+--------+\n",
      "|1         |2014-04-07 18:51:09.277|214536502|0       |\n",
      "|1         |2014-04-07 18:54:09.868|214536500|0       |\n",
      "|1         |2014-04-07 18:54:46.998|214536506|0       |\n",
      "+----------+-----------------------+---------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_RD.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T21:57:49.638091Z",
     "start_time": "2022-06-12T21:57:48.734945Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df_RD = df_RD.withColumn(\"Category\", when(col('Category') > 12, 'B').otherwise(col('Category')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standardize all categories outside of S, 0 and 1-12 to B for brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T21:58:55.323911Z",
     "start_time": "2022-06-12T21:57:49.645231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+--------+\n",
      "|Session_ID|           Timestamp|  Item_ID|Category|\n",
      "+----------+--------------------+---------+--------+\n",
      "|   6157803|2014-07-12 02:37:...|214845956|       B|\n",
      "|   6157803|2014-07-12 02:38:...|214845956|       B|\n",
      "+----------+--------------------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_RD.where(col(\"Session_ID\") == 6157803).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T21:58:55.604334Z",
     "start_time": "2022-06-12T21:58:55.330887Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc, lag\n",
    "\n",
    "w = Window.partitionBy(\"Session_ID\").orderBy(col(\"Timestamp\").desc())\n",
    "df_RD = df_RD.withColumn(\"next_Timestamp\", lag(df_RD.Timestamp).over(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use window & lag to get previous row's timestamp value by sorting timestamp in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T21:59:51.118796Z",
     "start_time": "2022-06-12T21:58:55.611140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+---------+--------+-----------------------+\n",
      "|Session_ID|Timestamp              |Item_ID  |Category|next_Timestamp         |\n",
      "+----------+-----------------------+---------+--------+-----------------------+\n",
      "|5274901   |2014-06-30 22:39:06.767|214691525|2       |null                   |\n",
      "|5274901   |2014-06-30 22:37:45.792|214850822|2       |2014-06-30 22:39:06.767|\n",
      "|5274901   |2014-06-30 22:34:45.691|214691587|S       |2014-06-30 22:37:45.792|\n",
      "|5274901   |2014-06-30 22:34:12.711|214840907|2       |2014-06-30 22:34:45.691|\n",
      "+----------+-----------------------+---------+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_RD.where(df_RD.Session_ID == 5274901).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T21:59:51.227505Z",
     "start_time": "2022-06-12T21:59:51.123171Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnull, unix_timestamp\n",
    "\n",
    "df_RD = df_RD.withColumn(\"Time_Spent (s)\", when(isnull(unix_timestamp(\"next_Timestamp\") - unix_timestamp(\"Timestamp\")), 0).otherwise(unix_timestamp(\"next_Timestamp\") - unix_timestamp(\"Timestamp\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-14T11:46:49.156054Z",
     "start_time": "2020-08-14T11:46:49.151068Z"
    }
   },
   "source": [
    "next_timestamp minus time_stamp = time spent on item. null values indicate the last item accessed in session. since the dataset offers no way to determine the time of exit, set as 0 and later exclude row when calculating average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T21:59:51.289339Z",
     "start_time": "2022-06-12T21:59:51.232808Z"
    }
   },
   "outputs": [],
   "source": [
    "df_RD = df_RD.sort(col(\"Session_ID\"), col(\"Timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T22:00:46.306404Z",
     "start_time": "2022-06-12T21:59:51.295324Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+---------+--------+-----------------------+--------------+\n",
      "|Session_ID|Timestamp              |Item_ID  |Category|next_Timestamp         |Time_Spent (s)|\n",
      "+----------+-----------------------+---------+--------+-----------------------+--------------+\n",
      "|5274901   |2014-06-30 22:34:12.711|214840907|2       |2014-06-30 22:34:45.691|33            |\n",
      "|5274901   |2014-06-30 22:34:45.691|214691587|S       |2014-06-30 22:37:45.792|180           |\n",
      "|5274901   |2014-06-30 22:37:45.792|214850822|2       |2014-06-30 22:39:06.767|81            |\n",
      "|5274901   |2014-06-30 22:39:06.767|214691525|2       |null                   |0             |\n",
      "+----------+-----------------------+---------+--------+-----------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_RD.where(df_RD.Session_ID == 5274901).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T22:01:41.377970Z",
     "start_time": "2022-06-12T22:00:46.310392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+---------+--------+-----------------------+--------------+\n",
      "|Session_ID|Timestamp              |Item_ID  |Category|next_Timestamp         |Time_Spent (s)|\n",
      "+----------+-----------------------+---------+--------+-----------------------+--------------+\n",
      "|5274901   |2014-06-30 22:34:12.711|214840907|2       |2014-06-30 22:34:45.691|33            |\n",
      "|5274901   |2014-06-30 22:34:45.691|214691587|S       |2014-06-30 22:37:45.792|180           |\n",
      "|5274901   |2014-06-30 22:37:45.792|214850822|2       |2014-06-30 22:39:06.767|81            |\n",
      "+----------+-----------------------+---------+--------+-----------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_RD_no_zeroes = df_RD.where(col(\"Time_Spent (s)\") != 0)\n",
    "df_RD_no_zeroes.where(df_RD_no_zeroes.Session_ID == 5274901).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter the 0s in time_spent column before calculating average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-12T22:03:41.706555Z",
     "start_time": "2022-06-12T22:01:41.381882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|Category|avg(Time_Spent (s))|\n",
      "+--------+-------------------+\n",
      "|       7|  176.2787246127175|\n",
      "|      11| 136.95635630224143|\n",
      "|       3| 115.27989871062425|\n",
      "|       8|  159.6770950770387|\n",
      "|       0| 145.48937745235582|\n",
      "|       5| 197.63863904487755|\n",
      "|       B| 173.52147913561848|\n",
      "|       6| 196.89169795888017|\n",
      "|       S| 147.20889172922796|\n",
      "|       9| 175.07878404053199|\n",
      "|       1| 166.26310856804616|\n",
      "|      10| 163.99088694946215|\n",
      "|       4|  165.0959717712642|\n",
      "|      12| 233.70240973901443|\n",
      "|       2|  177.1940657677661|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df_RD_no_zeroes.groupBy(\"Category\").agg(avg(\"Time_Spent (s)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average time spent in each category type"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
