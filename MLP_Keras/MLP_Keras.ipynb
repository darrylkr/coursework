{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:53:39.338941Z",
     "start_time": "2020-08-25T04:53:39.335949Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:53:40.831947Z",
     "start_time": "2020-08-25T04:53:40.828955Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:53:41.995833Z",
     "start_time": "2020-08-25T04:53:41.991843Z"
    }
   },
   "outputs": [],
   "source": [
    "col_names = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:53:42.713911Z",
     "start_time": "2020-08-25T04:53:42.709922Z"
    }
   },
   "outputs": [],
   "source": [
    "col_names = col_names + ['Wilderness_Area_{:02d}'.format(i+1) for i in range(4)] + ['Soil_Type_{:02d}'.format(i+1) for i in range(40)] + ['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:53:44.460239Z",
     "start_time": "2020-08-25T04:53:43.327270Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/covtype.data.gz\", names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:53:45.159369Z",
     "start_time": "2020-08-25T04:53:45.152387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(581012, 55)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:24:43.150658Z",
     "start_time": "2020-08-25T04:24:43.147666Z"
    }
   },
   "source": [
    "#### Normalize data to get the % for each Cover_Type class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:53:46.597521Z",
     "start_time": "2020-08-25T04:53:46.586550Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.364605\n",
       "2    0.487599\n",
       "3    0.061537\n",
       "4    0.004728\n",
       "5    0.016339\n",
       "6    0.029891\n",
       "7    0.035300\n",
       "Name: Cover_Type, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Cover_Type'].value_counts(normalize=True).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 70-30 split for train/test data for each Cover_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:53:49.036994Z",
     "start_time": "2020-08-25T04:53:48.569245Z"
    }
   },
   "outputs": [],
   "source": [
    "split = int((581012/100)*70)\n",
    "train = pd.concat([data[data.Cover_Type==1].sample(int(split*0.364605)), data[data.Cover_Type==2].sample(int(split*0.487599)), data[data.Cover_Type==3].sample(int(split*0.061537)), data[data.Cover_Type==4].sample(int(split*0.004728)), data[data.Cover_Type==5].sample(int(split*0.016339)), data[data.Cover_Type==6].sample(int(split*0.029891)), data[data.Cover_Type==7].sample(int(split*0.035300))])\n",
    "test = data.loc[~data.index.isin(train.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let validation dataset be subset of train data (50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:53:50.440239Z",
     "start_time": "2020-08-25T04:53:50.234789Z"
    }
   },
   "outputs": [],
   "source": [
    "split = len(train)/2 #50% of training data\n",
    "validation = pd.concat([train[train.Cover_Type==1].sample(int(split*0.364605)), train[train.Cover_Type==2].sample(int(split*0.487599)), train[train.Cover_Type==3].sample(int(split*0.061537)), train[train.Cover_Type==4].sample(int(split*0.004728)), train[train.Cover_Type==5].sample(int(split*0.016339)), train[train.Cover_Type==6].sample(int(split*0.029891)), train[train.Cover_Type==7].sample(int(split*0.035300))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:53:51.028665Z",
     "start_time": "2020-08-25T04:53:51.024676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((406703, 55), (174309, 55))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Z-score standardization for the continuous variables\n",
    "#### Take each continuous column value and subtract by the mean then divide by standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:53:52.609436Z",
     "start_time": "2020-08-25T04:53:52.603452Z"
    }
   },
   "outputs": [],
   "source": [
    "def zScoreStandardization(coverType):\n",
    "    mean = train.iloc[:,:10][train['Cover_Type'] == coverType].mean()\n",
    "    std = train.iloc[:,:10][train['Cover_Type'] == coverType].std()\n",
    "    train.loc[train['Cover_Type'] == coverType, :'Horizontal_Distance_To_Fire_Points'] = (train.loc[train['Cover_Type'] == coverType, :'Horizontal_Distance_To_Fire_Points'] - mean) / std\n",
    "    \n",
    "    mean = validation.iloc[:,:10][validation['Cover_Type'] == coverType].mean()\n",
    "    std = validation.iloc[:,:10][validation['Cover_Type'] == coverType].std()\n",
    "    validation.loc[validation['Cover_Type'] == coverType, :'Horizontal_Distance_To_Fire_Points'] = (validation.loc[validation['Cover_Type'] == coverType, :'Horizontal_Distance_To_Fire_Points'] - mean) / std\n",
    "    \n",
    "    mean = test.iloc[:,:10][test['Cover_Type'] == coverType].mean()\n",
    "    std = test.iloc[:,:10][test['Cover_Type'] == coverType].std()\n",
    "    test.loc[test['Cover_Type'] == coverType, :'Horizontal_Distance_To_Fire_Points'] = (test.loc[test['Cover_Type'] == coverType, :'Horizontal_Distance_To_Fire_Points'] - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:23:01.781905Z",
     "start_time": "2020-08-25T04:23:01.778912Z"
    }
   },
   "source": [
    "#### Run through the continuous variables columns using a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:53:56.476090Z",
     "start_time": "2020-08-25T04:53:53.953839Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(1, 8):\n",
    "    zScoreStandardization(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding of the cover_type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:54:02.223712Z",
     "start_time": "2020-08-25T04:54:01.965403Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train.drop(\"Cover_Type\", axis=1).values\n",
    "y_train = pd.get_dummies(train[\"Cover_Type\"]).values\n",
    "X_valid = validation.drop(\"Cover_Type\", axis=1).values\n",
    "y_valid = pd.get_dummies(validation[\"Cover_Type\"]).values\n",
    "X_test = test.drop(\"Cover_Type\", axis=1).values\n",
    "y_test = pd.get_dummies(test[\"Cover_Type\"]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:54:02.846048Z",
     "start_time": "2020-08-25T04:54:02.842058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((406703, 54), (406703, 7))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:15:34.826392Z",
     "start_time": "2020-08-25T04:15:34.823399Z"
    }
   },
   "source": [
    "### Self-implemented MLP model (Plain TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 hidden layer consisting of 80 neurons\n",
    "#### Output layer has 7 neurons because there are 7 Cover_Types.\n",
    "#### Input layer = 54 feature columns.\n",
    "#### 2 bias neurons, 1 at input layer and 1 at hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:54:04.800817Z",
     "start_time": "2020-08-25T04:54:04.795831Z"
    }
   },
   "outputs": [],
   "source": [
    "n_neurons_h = 80 # hidden layer neurons\n",
    "n_neurons_out = 7 # output layer neurons\n",
    "n_features = 54 # number of features\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "y_true = tf.placeholder(tf.float32, shape=(None , n_neurons_out), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:54:05.324416Z",
     "start_time": "2020-08-25T04:54:05.312448Z"
    }
   },
   "outputs": [],
   "source": [
    "#original\n",
    "W1 = tf.get_variable(\"weights1\", dtype=tf.float32, initializer=tf.zeros((n_features, n_neurons_h)))\n",
    "b1 = tf.get_variable(\"bias1\", dtype=tf.float32, initializer=tf.zeros((n_neurons_h)))\n",
    "W2 = tf.get_variable(\"weights2\", dtype=tf.float32, initializer=tf.zeros((n_neurons_h, n_neurons_out)))\n",
    "b2 = tf.get_variable(\"bias2\", dtype=tf.float32, initializer=tf.zeros((n_neurons_out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden layer uses sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:54:06.596014Z",
     "start_time": "2020-08-25T04:54:06.591028Z"
    }
   },
   "outputs": [],
   "source": [
    "# make the network\n",
    "h = tf.nn.sigmoid(tf.matmul(X, W1)+ b1)\n",
    "z = tf.matmul(h, W2) + b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output layer uses softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:54:08.093009Z",
     "start_time": "2020-08-25T04:54:08.085030Z"
    }
   },
   "outputs": [],
   "source": [
    "y_hat = tf.exp(z) / tf.reduce_sum(tf.exp(z))\n",
    "cross_entropy = -y_true * tf.log(y_hat)\n",
    "base_cost = tf.reduce_mean(cross_entropy)\n",
    "reg_cost = tf.reduce_sum(tf.pow(W1, 2.0)) + tf.reduce_sum(tf.pow(W2, 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:54:08.517873Z",
     "start_time": "2020-08-25T04:54:08.514880Z"
    }
   },
   "outputs": [],
   "source": [
    "#helper function\n",
    "def get_indexes_max_value(l):\n",
    "    max_value = max(l)\n",
    "    if l.count(max_value) > 1:\n",
    "        return [i for i, x in enumerate(l) if x == max(l)]\n",
    "    else:\n",
    "        return l.index(max(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypertune method to decide on the best L2 Regularization parameter value among 5 defined values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:54:09.441401Z",
     "start_time": "2020-08-25T04:54:09.434420Z"
    }
   },
   "outputs": [],
   "source": [
    "def hypertuneL2(base_cost, reg_cost):\n",
    "    init = tf.global_variables_initializer()\n",
    "    n_epochs = 20\n",
    "\n",
    "    accuracyList = []\n",
    "    test_loss = []\n",
    "    l2Grid = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    index = 0\n",
    "\n",
    "    for i in range(len(l2Grid)):\n",
    "        cost = base_cost + l2Grid[i] * reg_cost\n",
    "        learning_rate = 0.04\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(cost)\n",
    "        n_epochs = 20\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            init.run()\n",
    "            for epoch in range(n_epochs):\n",
    "                training_cost, _ = sess.run([cost, training_op], feed_dict={X: X_valid, y_true: y_valid})\n",
    "                valid_cost = sess.run(cost, feed_dict={X: X_test, y_true: y_test})\n",
    "\n",
    "                if (epoch == n_epochs-1):\n",
    "                    test_loss.append(valid_cost)\n",
    "\n",
    "            preds = tf.nn.softmax(z)\n",
    "            correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_true, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            value = accuracy.eval({X: X_test, y_true: y_test})\n",
    "            accuracyList.append(value)\n",
    "            sess.close()\n",
    "\n",
    "    a = []\n",
    "    a.append(get_indexes_max_value(accuracyList))\n",
    "    if (len(a) == 1):\n",
    "        index = accuracyList.index(max(accuracyList))\n",
    "    else:\n",
    "        min_loss = []\n",
    "        for i in a:\n",
    "            min_loss.append(test_loss[i])\n",
    "        index = min_loss.index(min(min_loss))\n",
    "        \n",
    "    return l2Grid[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:54:32.126706Z",
     "start_time": "2020-08-25T04:54:10.855618Z"
    }
   },
   "outputs": [],
   "source": [
    "l2_param = hypertuneL2(base_cost, reg_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:54:32.132690Z",
     "start_time": "2020-08-25T04:54:32.127703Z"
    }
   },
   "outputs": [],
   "source": [
    "cost = base_cost + l2_param * reg_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:54:32.185549Z",
     "start_time": "2020-08-25T04:54:32.133688Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.04\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Entrophy to evaluate on model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:54:39.526907Z",
     "start_time": "2020-08-25T04:54:32.186546Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_cross_entropy: 2.123107671737671, Test_cross_entropy: 1.9981073141098022\n",
      "Train_cross_entropy: 2.1191439628601074, Test_cross_entropy: 1.9943419694900513\n",
      "Train_cross_entropy: 2.115377426147461, Test_cross_entropy: 1.9907639026641846\n",
      "Train_cross_entropy: 2.1118013858795166, Test_cross_entropy: 1.987370252609253\n",
      "Train_cross_entropy: 2.1084041595458984, Test_cross_entropy: 1.9841485023498535\n",
      "Train_cross_entropy: 2.10518479347229, Test_cross_entropy: 1.9810963869094849\n",
      "Train_cross_entropy: 2.102130174636841, Test_cross_entropy: 1.9782030582427979\n",
      "Train_cross_entropy: 2.099238634109497, Test_cross_entropy: 1.9754652976989746\n",
      "Train_cross_entropy: 2.096498966217041, Test_cross_entropy: 1.9728730916976929\n",
      "Train_cross_entropy: 2.09390926361084, Test_cross_entropy: 1.9704217910766602\n",
      "Train_cross_entropy: 2.091456890106201, Test_cross_entropy: 1.9681044816970825\n",
      "Train_cross_entropy: 2.089139223098755, Test_cross_entropy: 1.9659147262573242\n",
      "Train_cross_entropy: 2.0869476795196533, Test_cross_entropy: 1.9638456106185913\n",
      "Train_cross_entropy: 2.0848782062530518, Test_cross_entropy: 1.9618908166885376\n",
      "Train_cross_entropy: 2.0829248428344727, Test_cross_entropy: 1.960045337677002\n",
      "Train_cross_entropy: 2.081080436706543, Test_cross_entropy: 1.9583032131195068\n",
      "Train_cross_entropy: 2.0793375968933105, Test_cross_entropy: 1.9566599130630493\n",
      "Train_cross_entropy: 2.0776937007904053, Test_cross_entropy: 1.9551085233688354\n",
      "Train_cross_entropy: 2.0761430263519287, Test_cross_entropy: 1.9536458253860474\n",
      "Train_cross_entropy: 2.074679374694824, Test_cross_entropy: 1.9522652626037598\n",
      "[Test Accuracy] : 0.48758814\n"
     ]
    }
   ],
   "source": [
    "# execute the model\n",
    "init = tf.global_variables_initializer()\n",
    "n_epochs = 20\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        training_cost, _ = sess.run([cost, training_op], feed_dict={X: X_train, y_true: y_train})\n",
    "        test_cost = sess.run(cost, feed_dict={X: X_test, y_true: y_test})\n",
    "        print(f\"Train_cross_entropy: {training_cost}, Test_cross_entropy: {test_cost}\")\n",
    "    \n",
    "    # Test model\n",
    "    preds = tf.nn.softmax(z)  # Apply softmax to logits\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_true, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"[Test Accuracy] :\", accuracy.eval({X: X_test, y_true: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras MLP model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:54:39.530896Z",
     "start_time": "2020-08-25T04:54:39.527904Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 hidden layer consisting of 80 neurons\n",
    "#### Output layer has 7 neurons because there are 7 Cover_Types.\n",
    "#### Input layer = 54 feature columns.\n",
    "#### 2 bias neurons, 1 at input layer and 1 at hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:54:39.535883Z",
     "start_time": "2020-08-25T04:54:39.531894Z"
    }
   },
   "outputs": [],
   "source": [
    "n_neurons_out = 7 # number of neurons in output layer\n",
    "n_neurons_h = 80 # number of neurons in hidden layer \n",
    "n_features = 54 # number of neurons(features)\n",
    "n_epochs = 10\n",
    "learning_rate = 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:54:40.659875Z",
     "start_time": "2020-08-25T04:54:39.536880Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = tf.dtypes.cast(X_train, tf.float32)\n",
    "y_train = tf.dtypes.cast(y_train, tf.float32)\n",
    "X_valid = tf.dtypes.cast(X_valid, tf.float32)\n",
    "y_valid = tf.dtypes.cast(y_valid, tf.float32)\n",
    "X_test = tf.dtypes.cast(X_test, tf.float32)\n",
    "y_test = tf.dtypes.cast(y_test, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron in Keras\n",
    "#### sigmoid activation function for hidden layer, softmax activation function for output layer\n",
    "#### Hypertune method to decide on the best L2 Regularization parameter value among 5 defined values (adapted to keras api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:54:40.666856Z",
     "start_time": "2020-08-25T04:54:40.660872Z"
    }
   },
   "outputs": [],
   "source": [
    "def kerasL2():\n",
    "    L2Grid = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    test_loss = []\n",
    "    max_accuracy = []\n",
    "    for i in range(len(L2Grid)):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(Dense(n_neurons_h, activation=\"sigmoid\", kernel_regularizer=tf.keras.regularizers.l2(L2Grid[i])))\n",
    "        model.add(Dense(n_neurons_out, activation=\"softmax\", kernel_regularizer=tf.keras.regularizers.l2(L2Grid[i])))\n",
    "        model.compile(optimizer=tf.train.GradientDescentOptimizer(learning_rate), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "        model.fit(X_valid, y_valid, epochs=n_epochs, steps_per_epoch=20, verbose=0)\n",
    "        loss, accuracy = model.evaluate(X_test, y_test, steps=1)\n",
    "        test_loss.append(loss)\n",
    "        max_accuracy.append(accuracy)\n",
    "    \n",
    "    a = []\n",
    "    a.append(get_indexes_max_value(max_accuracy))\n",
    "    if (len(a) == 1):\n",
    "        index = max_accuracy.index(max(max_accuracy))\n",
    "    else:\n",
    "        min_loss = []\n",
    "        for i in a:\n",
    "            min_loss.append(test_loss[i])\n",
    "        index = min_loss.index(min(min_loss))\n",
    "        \n",
    "    return L2Grid[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:56:12.776415Z",
     "start_time": "2020-08-25T04:54:40.667854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Yongx\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    }
   ],
   "source": [
    "l2_param = kerasL2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:36:32.310193Z",
     "start_time": "2020-08-25T04:36:32.307200Z"
    }
   },
   "source": [
    "#### Model compiler uses categorical cross entropy for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:56:12.785391Z",
     "start_time": "2020-08-25T04:56:12.777412Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(n_neurons_h, activation=\"sigmoid\", kernel_regularizer=tf.keras.regularizers.l2(l2_param)))\n",
    "model.add(Dense(n_neurons_out, activation=\"softmax\", kernel_regularizer=tf.keras.regularizers.l2(l2_param)))\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(learning_rate), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20 batches of samples to use in one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:56:43.496223Z",
     "start_time": "2020-08-25T04:56:12.786388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20 samples\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 2s 123ms/step - batch: 9.5000 - size: 1.0000 - loss: 8.1475 - acc: 0.4499\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 2s 123ms/step - batch: 9.5000 - size: 1.0000 - loss: 6.1658 - acc: 0.4877\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 2s 124ms/step - batch: 9.5000 - size: 1.0000 - loss: 4.8125 - acc: 0.4876\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 2s 124ms/step - batch: 9.5000 - size: 1.0000 - loss: 3.8343 - acc: 0.4876\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 2s 122ms/step - batch: 9.5000 - size: 1.0000 - loss: 3.1256 - acc: 0.4876\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 2s 125ms/step - batch: 9.5000 - size: 1.0000 - loss: 2.6118 - acc: 0.4876\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 2s 124ms/step - batch: 9.5000 - size: 1.0000 - loss: 2.2390 - acc: 0.4876\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 2s 123ms/step - batch: 9.5000 - size: 1.0000 - loss: 1.9686 - acc: 0.4876\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 2s 123ms/step - batch: 9.5000 - size: 1.0000 - loss: 1.7723 - acc: 0.4876\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 2s 123ms/step - batch: 9.5000 - size: 1.0000 - loss: 1.6297 - acc: 0.4876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e79e144448>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=n_epochs, steps_per_epoch=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss, Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T04:56:46.778441Z",
     "start_time": "2020-08-25T04:56:43.497221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5698704719543457, 0.48760396]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, steps=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
